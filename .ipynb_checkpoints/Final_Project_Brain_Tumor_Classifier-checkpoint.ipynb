{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Image segmentation for detecting brain tumor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous kernel I made a CNN model to classify if a brain has tumor or not. The accuracy of the model was really good. But in this kernel I am going to use Image segmentation by U-net CNN model to determain the brain's tumor in yes cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** My analysis take the following steps:**\n",
    "\n",
    "1- Making Mask\n",
    "\n",
    "2- Preparing data\n",
    "\n",
    "3- Building U-net CNN model\n",
    "\n",
    "\n",
    "4- Training the model\n",
    "\n",
    "5-Evalutation Model on some training data\n",
    "\n",
    "\n",
    "6-Evaluation model on the test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Making Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is mask? **\n",
    "\n",
    "Image segmentation creates a pixel-wise mask for each object in the image. This technique gives us a far more granular understanding of the object(s) in the image.\n",
    "\n",
    "[![enter image description here][1]][1]\n",
    "\n",
    "  [1]: https://i.stack.imgur.com/sXrwS.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rigth photo, each item bounded in red,blue and green curves is called a mask. Image segmentation creates a pixel-wise mask for each object in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project tumors in yes images are masks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to create masks for our train data set?**\n",
    "\n",
    "There are many ways to make masks for a data set. You can find many Github sources. In this project I used labelbox. Labelbox is a very powerfull software for making masks. I uploudes masks for all train data. You can find them in the train folder. (Note that I don't have medical background so masks migth not be quite exact). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some masks and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4c2d027ddc83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mTRAIN_PATH\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../input/training-data-and-masks/train/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTRAIN_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mid_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "TRAIN_PATH='../data/'\n",
    "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "for n, id_ in enumerate(train_ids):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img = plt.imread(path + '/images/' + id_ )[:,:,:3]\n",
    "    img1 = plt.imread(path + '/masks/' + id_ )[:,:,:3]\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1,2, 1)\n",
    "    plt.imshow(np.rot90(img,2))\n",
    "    f.add_subplot(1,2, 2)\n",
    "    plt.imshow(np.rot90(img1,2))\n",
    "    plt.show(block=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preparing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "print('Getting and resizing training images ... ')\n",
    "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "        \n",
    "# Re-sizing our training images to 128 x 128\n",
    "# Note sys.stdout prints info that can be cleared unlike print.\n",
    "# Using TQDM allows us to create progress bars\n",
    "sys.stdout.flush()\n",
    "\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img = plt.imread(path + '/images/' + id_ )[:,:,:IMG_CHANNELS]\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_train[n] = img\n",
    "    mask = plt.imread(path + '/masks/' + id_)[:,:,:1]\n",
    "    mask=resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    Y_train[n] = mask\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at images and masks after preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "x, y = 12,4\n",
    "for i in range(y):  \n",
    "    for j in range(x):\n",
    "        plt.subplot(y*2, x, i*2*x+j+1)\n",
    "        pos = i*5 + j*8\n",
    "        plt.imshow(X_train[pos])\n",
    "        plt.title('Image #{}'.format(pos))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(y*2, x, (i*2+1)*x+j+1)\n",
    "        \n",
    "        #We display the associated mask we just generated above with the training image\n",
    "        plt.imshow(np.squeeze(Y_train[pos]))\n",
    "        plt.title('Mask #{}'.format(pos))\n",
    "        \n",
    "        plt.axis('off')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Building U-net CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is u-net CNN model?**\n",
    "\n",
    "U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 × 512 image takes less than a second on a modern GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why we call it U-net network? **\n",
    "\n",
    "The reason behind why it is named U-Net is because of the shape of its architecture, which is the letter “U”. The architecture contains two paths. The left part of the letter “U” is called an encoder and the right part is called a decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is encoder and decoder?**\n",
    "\n",
    "The encoder is the first half in the architecture diagram (Figure below). It usually is a pre-trained classification network like VGG/ResNet where you apply convolution blocks followed by a maxpool downsampling to encode the input image into feature representations at multiple different levels.\n",
    "\n",
    "The decoder is the second half of the architecture. The goal is to semantically project the discriminative features (lower resolution) learnt by the encoder onto the pixel space (higher resolution) to get a dense classification. The decoder consists of upsampling and concatenation followed by regular convolution operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![enter image description here][1]][1]\n",
    "\n",
    "\n",
    "  [1]: https://i.stack.imgur.com/hMQjJ.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is encoder and decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import warnings #\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.morphology import label\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building U-net CNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build U-Net model\n",
    "# Note we make our layers varaibles so that we can concatenate or stack\n",
    "# This is required so that we can re-create our U-Net Model\n",
    "\n",
    "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "s = Lambda(lambda x: x / 255) (inputs)\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "c2 = Dropout(0.1) (c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = Dropout(0.2) (c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = Dropout(0.2) (c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "c5 = Dropout(0.3) (c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "c6 = Dropout(0.2) (c6)\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "c7 = Dropout(0.2) (c7)\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "c8 = Dropout(0.1) (c8)\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "c9 = Dropout(0.1) (c9)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "# Note our output is effectively a mask of 128 x 128 \n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "model_path = \"nuclei_finder_unet_1.h5\"\n",
    "# Initialize our callbacks\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# Fit our model \n",
    "results = model.fit(X_train, Y_train, validation_split=0.1,\n",
    "                    batch_size=16, epochs=10,callbacks=[earlystop, checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(results.history['accuracy'])\n",
    "    plt.plot(results.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # \"Loss\"\n",
    "    plt.plot(results.history['loss'])\n",
    "    plt.plot(results.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the traing data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('nuclei_finder_unet_1.h5', \n",
    "                   custom_objects={'my_iou_metric': my_iou_metric})\n",
    "\n",
    "# the first 90% was used for training\n",
    "preds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
    "\n",
    "\n",
    "#preds_test = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the evalution of the data on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting our predicted masks\n",
    "ix = random.randint(0, 120)\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# Our original training image\n",
    "plt.subplot(131)\n",
    "imshow(X_train[ix])\n",
    "plt.title(\"Image\")\n",
    "\n",
    "# Our original combined mask  \n",
    "plt.subplot(132)\n",
    "imshow(np.squeeze(Y_train[ix]))\n",
    "plt.title(\"Mask\")\n",
    "\n",
    "# The mask our U-Net model predicts\n",
    "plt.subplot(133)\n",
    "imshow(np.squeeze(preds_train_t[ix] > 0.5))\n",
    "plt.title(\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evalute the model on the test data. First we read test images from testData folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "data=[]\n",
    "for dirname, _, filenames in os.walk('../input/testdata/test'):\n",
    "    for filename in filenames:\n",
    "        img=cv2.imread(os.path.join(dirname, filename))\n",
    "        img=cv2.resize(img, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "        if img is not None:\n",
    "                data.append(img)\n",
    "        \n",
    "data1=np.array(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction of the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(data1, verbose=1)\n",
    "\n",
    "\n",
    "#preds_test = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "preds_test_t = (preds_train > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(data1, verbose=1)\n",
    "\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the evalation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(data)):\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1,2, 1)\n",
    "    plt.imshow(np.rot90(data[n],2))\n",
    "    f.add_subplot(1,2, 2)\n",
    "    plt.imshow(np.rot90(np.squeeze(preds_test_t[n]),2))\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "    \n",
    "In this project I used U-net CNN for Image segmentation of brain tumor. The result is good on the test data but it can be better by improving the model. For example creating accuracy function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
